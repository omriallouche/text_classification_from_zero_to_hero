{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this presentation, we will use FLAIR: https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
      "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
      "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
      "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
      "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
      "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
      "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
      "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
      "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
      "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
      "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
      "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
      "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
      "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
      "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
      "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
      "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
      "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
      "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
      "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02])\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
      "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
      "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
      "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
      "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
      "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
      "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
      "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
      "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
      "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
      "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
      "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
      "        -0.3924, -0.2339,  0.4730, -0.0288])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "glove_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Compare the embeddings obtained using GloVe for the same word in different context (ie different sentences). Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's average the vectors into a single vector that would represent our entire document, and use it for classification. We'll build a Logistic Regression classifier on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    sentence = Sentence(sentence)\n",
    "    glove_embedding.embed(sentence)\n",
    "    sentence_embedding = np.mean( [np.array(token.embedding) for token in sentence], axis=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48264474,  0.33375996,  0.348696  , -0.5163    ,  0.191962  ,\n",
       "        0.12714759,  0.013061  ,  0.1766614 , -0.1873308 , -0.093839  ,\n",
       "        0.0488024 , -0.0484856 ,  0.314986  ,  0.031634  ,  0.2535662 ,\n",
       "       -0.059972  ,  0.38505   ,  0.06304   ,  0.027378  ,  0.06385148,\n",
       "       -0.1046188 ,  0.131214  ,  0.39698398,  0.0049592 ,  0.48706597,\n",
       "        0.27059498,  0.0188544 , -0.780686  , -0.160654  , -0.0207716 ,\n",
       "       -0.2985124 ,  0.521548  ,  0.371312  ,  0.0037584 ,  0.24874802,\n",
       "        0.3579286 , -0.187218  ,  0.484008  ,  0.1211252 ,  0.0338024 ,\n",
       "       -0.32039762, -0.578998  ,  0.1858078 , -0.27883598,  0.07773139,\n",
       "       -0.14281002,  0.23905559, -0.13043599, -0.1817726 , -0.49833995,\n",
       "       -0.10820474, -0.30922002,  0.285602  ,  1.1599319 , -0.49102196,\n",
       "       -2.58022   ,  0.021746  ,  0.043806  ,  1.479552  ,  0.427112  ,\n",
       "       -0.02804599,  0.67730397, -0.0862168 ,  0.305978  ,  1.0884    ,\n",
       "       -0.21497002,  0.2661428 , -0.022402  ,  0.3063696 , -0.2959406 ,\n",
       "       -0.04430168, -0.124852  , -0.3095786 , -0.37071198,  0.2658556 ,\n",
       "        0.01379652,  0.1408166 ,  0.0865896 , -0.61099005,  0.25847322,\n",
       "        0.569286  ,  0.05893   , -0.656514  ,  0.1472954 , -0.85989   ,\n",
       "       -0.23183   ,  0.043911  , -0.21173999,  0.36706018,  0.13562599,\n",
       "       -0.0454518 , -0.6721238 , -0.32438502,  0.53062   , -0.48571223,\n",
       "       -0.01712   , -0.4413198 , -0.288836  ,  0.69392604,  0.3024668 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_embedding('The grass is green .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>data_processed</th>\n",
       "      <th>num_chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Does anyone have the scoop on Scot Erickson?  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>does anyone have the scoop on scot erickson ? ...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nNot particularly *in* the World Series. Duri...</td>\n",
       "      <td>0</td>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>not particularly *in* the world series . durin...</td>\n",
       "      <td>3145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nI think the three-headed GM's guiding princi...</td>\n",
       "      <td>1</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>i think the three-headed gm's guiding principl...</td>\n",
       "      <td>959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\n\\nA suggestion: cameras panning over plant...</td>\n",
       "      <td>2</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>a suggestion: cameras panning over planted aut...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nDon't you Americans study history...the Fren...</td>\n",
       "      <td>1</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>don't you americans study history . . .the fre...</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target  \\\n",
       "0  Does anyone have the scoop on Scot Erickson?  ...       0   \n",
       "1  \\nNot particularly *in* the World Series. Duri...       0   \n",
       "2  \\nI think the three-headed GM's guiding princi...       1   \n",
       "3  \\n\\n\\nA suggestion: cameras panning over plant...       2   \n",
       "4  \\nDon't you Americans study history...the Fren...       1   \n",
       "\n",
       "          target_name                                     data_processed  \\\n",
       "0  rec.sport.baseball  does anyone have the scoop on scot erickson ? ...   \n",
       "1  rec.sport.baseball  not particularly *in* the world series . durin...   \n",
       "2    rec.sport.hockey  i think the three-headed gm's guiding principl...   \n",
       "3  talk.politics.guns  a suggestion: cameras panning over planted aut...   \n",
       "4    rec.sport.hockey  don't you americans study history . . .the fre...   \n",
       "\n",
       "   num_chars  \n",
       "0         82  \n",
       "1       3145  \n",
       "2        959  \n",
       "3        151  \n",
       "4        207  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('20newsgroups.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [get_sentence_embedding(x) for x in df['data_processed']]\n",
    "vectors = np.array(vectors)\n",
    "y_truth = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = linear_model.LogisticRegression(C=1e5)\n",
    "clf.fit(vectors, y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8789065712873826"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own Word Vectors\n",
    "It's very easy to train our own word vectors based on our custom task. This can lead to an increase in performance if our domain is different from that used for training common word vectors (usually Wikipedia).  \n",
    "\n",
    "We will train word vectors using the Gensim package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "types_to_remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups = fetch_20newsgroups(remove=types_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    txt = txt.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').replace('?', ' ?').replace('.', ' .').replace(',', ' ,')\n",
    "    txt = txt.lower().strip()\n",
    "    txt = txt.split(' ')\n",
    "    txt = \" \".join([w for w in txt if w!=''])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v = gensim.models.Word2Vec([preprocess_text(s).split() for s in newsgroups.data], iter=50, sg=1, min_count=5, size=100, window=3, workers=7)\n",
    "w2v.init_sims(replace=True) # frees memory of word vectors but prevents further training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_words(words_to_check = ['love', 'hate', 'lonely', 'heartache', 'success', 'guitar', 'god', 'beer', 'gun', 'police']):\n",
    "    for word in words_to_check:\n",
    "        print(word, ' -> ')\n",
    "        try:\n",
    "            print('\\n'.join(['\\t{} ({:.2f}), '.format(tup[0], tup[1]) for tup in w2v.wv.similar_by_word(word, topn=5)]))\n",
    "        except:\n",
    "            pass\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love  -> \n",
      "\tdespise (0.61), \n",
      "\tmisread (0.57), \n",
      "\tmourning (0.56), \n",
      "\thate (0.55), \n",
      "\ttrust (0.54), \n",
      "\n",
      "hate  -> \n",
      "\tloathe (0.57), \n",
      "\tlove (0.55), \n",
      "\tgeneralize (0.53), \n",
      "\tprefer (0.53), \n",
      "\thating (0.51), \n",
      "\n",
      "lonely  -> \n",
      "\n",
      "heartache  -> \n",
      "\n",
      "success  -> \n",
      "\tdearly (0.51), \n",
      "\tresults: (0.50), \n",
      "\tincidence (0.50), \n",
      "\tmarkedly (0.50), \n",
      "\tconcern (0.49), \n",
      "\n",
      "guitar  -> \n",
      "\tmoe (0.52), \n",
      "\tpuzzle (0.49), \n",
      "\tcorrectable (0.48), \n",
      "\ttetris (0.48), \n",
      "\trams (0.47), \n",
      "\n",
      "god  -> \n",
      "\tjesus (0.76), \n",
      "\tchrist (0.73), \n",
      "\twhosoever (0.72), \n",
      "\teternal (0.70), \n",
      "\tgod's (0.70), \n",
      "\n",
      "beer  -> \n",
      "\tdrinking (0.52), \n",
      "\tsmoked (0.52), \n",
      "\ttalon (0.49), \n",
      "\tbackyard (0.49), \n",
      "\tcats (0.49), \n",
      "\n",
      "gun  -> \n",
      "\tfirearms (0.66), \n",
      "\thomicide (0.65), \n",
      "\thandgun (0.64), \n",
      "\thandguns (0.63), \n",
      "\tguns (0.61), \n",
      "\n",
      "police  -> \n",
      "\tcops (0.63), \n",
      "\tcriminal (0.62), \n",
      "\tprecinct (0.60), \n",
      "\taffidavit (0.60), \n",
      "\tlegislature (0.57), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to the GloVe embeddings from Flair. Flair doesn't provide a way to get the most similar vectors, but we can implement this in a naive way ourselves. We'll create a dictionary that maps a word from our data to its word embedding, and then given a query, will compute the cosine distance of it to all other words and return the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "d = df['data_processed'].values\n",
    "vocabulary = Counter([w for sent in d for w in sent.split()])\n",
    "TH = 5\n",
    "vocabulary = {k:v for k,v in vocabulary.most_common() if v>TH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = {}\n",
    "for w in vocabulary.keys():\n",
    "    sentence = Sentence(w)\n",
    "    glove_embedding.embed(sentence)\n",
    "    for token in sentence:\n",
    "        word_embeddings[token.text] = np.array(token.embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "narimanov       -0.377583\n",
       "ahola           -0.359337\n",
       "akgun           -0.354997\n",
       "lssu            -0.340201\n",
       "basim           -0.326174\n",
       "macoun          -0.326096\n",
       "akhalkalaki     -0.322035\n",
       "gunduz          -0.321786\n",
       "davidsson       -0.320443\n",
       "aslin           -0.317507\n",
       "bobbs-merrill   -0.312970\n",
       "khmylev         -0.311811\n",
       "villalta        -0.306674\n",
       "babych          -0.305737\n",
       "microdistrict   -0.304121\n",
       "anania          -0.303885\n",
       ".067            -0.302436\n",
       "puppa           -0.302278\n",
       "bortnick        -0.295164\n",
       "snd             -0.293724\n",
       "light-hitting   -0.293436\n",
       "dolezal         -0.290676\n",
       "basar           -0.289941\n",
       "cuyler          -0.289331\n",
       "erivan          -0.286078\n",
       "gaudreau        -0.284338\n",
       "stanky          -0.283377\n",
       "fedyk           -0.282819\n",
       "gurvitz         -0.282730\n",
       "18:39           -0.282158\n",
       "                   ...   \n",
       "someone          0.576079\n",
       "talk             0.578374\n",
       "feel             0.578664\n",
       "racial           0.579516\n",
       "remember         0.583630\n",
       "kind             0.585443\n",
       "do               0.588497\n",
       "why              0.588818\n",
       "forget           0.589449\n",
       "yes              0.589554\n",
       "believe          0.589611\n",
       "violence         0.590331\n",
       "?                0.590485\n",
       "anything         0.593559\n",
       "you              0.595325\n",
       "know             0.598443\n",
       "afraid           0.599686\n",
       "think            0.602174\n",
       "shame            0.603166\n",
       "anybody          0.604446\n",
       "want             0.605534\n",
       "motivated        0.608081\n",
       "crime            0.613874\n",
       "bigotry          0.622341\n",
       "fear             0.628029\n",
       "anyone           0.637533\n",
       "racism           0.641139\n",
       "racist           0.678473\n",
       "hatred           0.704508\n",
       "hate             1.000000\n",
       "Length: 10351, dtype: float32"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'hate'\n",
    "cos_sim = cosine_similarity(word_embeddings[query].reshape(1, -1), np.array([v for k,v in word_embeddings.items()]))[0]\n",
    "pd.Series(cos_sim, index=word_embeddings.keys()).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_from_custom_w2v(sentence):\n",
    "    l = w2v.wv.get_vector('the').shape\n",
    "    sentence_embedding = np.mean( [w2v.wv.get_vector(token) if token in w2v.wv.vocab else np.zeros(l) for token in sentence.split()], axis=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [get_sentence_embedding_from_custom_w2v(x) for x in df['data_processed']]\n",
    "vectors = np.array(vectors)\n",
    "y_truth = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = linear_model.LogisticRegression(C=1e5)\n",
    "clf.fit(vectors, y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = clf.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.838586471495323"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
