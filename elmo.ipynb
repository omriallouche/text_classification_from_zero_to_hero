{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "ELMo learns contextualized word vectors by running the text through a deep recurrent network.  \n",
    "ELMo is actually an algorithm for unsupervised learning and does not make any use of the labels we have for our text classification task. The authors do show that contextualized word vectors obtained using ELMo increase text classification performance in a large array of tasks. Let's see if we see a significant gain in our case!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are good examples of using ELMo in both [the AllenNLP github repo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) and [this AnalyticsVidhya post](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article). In this guide we'll use the python package [Flair](https://github.com/zalandoresearch/flair) to get ELMo embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embeddings with ELMo in Flair\n",
    "In Flair, you init a `Sentence` object given the tokens seperated by spaces.  \n",
    "Sentence has a few useful attributes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: allennlp in c:\\programdata\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: requests>=2.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (2.9.0)\n",
      "Requirement already satisfied: tensorboardX>=1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.9)\n",
      "Requirement already satisfied: editdistance in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.5.3)\n",
      "Requirement already satisfied: flask-cors>=3.0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (3.0.8)\n",
      "Requirement already satisfied: unidecode in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.1.1)\n",
      "Requirement already satisfied: responses>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.10.6)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.2.1)\n",
      "Requirement already satisfied: conllu==1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.3.1)\n",
      "Requirement already satisfied: flaky in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (3.6.1)\n",
      "Requirement already satisfied: ftfy in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (4.4.3)\n",
      "Requirement already satisfied: gevent>=1.3.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: flask>=1.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.0.2)\n",
      "Requirement already satisfied: word2number>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.1)\n",
      "Requirement already satisfied: tqdm>=4.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (4.31.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (2018.4)\n",
      "Requirement already satisfied: pytorch-transformers==1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.1.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (3.0.3)\n",
      "Requirement already satisfied: jsonpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.2)\n",
      "Requirement already satisfied: numpydoc>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.8.0)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (3.4)\n",
      "Requirement already satisfied: spacy<2.2,>=2.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (2.1.8)\n",
      "Requirement already satisfied: pytest in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (4.3.1)\n",
      "Requirement already satisfied: overrides in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (2.0)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.3.0+cpu)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.9.110)\n",
      "Requirement already satisfied: pytorch-pretrained-bert>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.6.2)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (1.16.2)\n",
      "Requirement already satisfied: parsimonious>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from allennlp) (0.8.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.18->allennlp) (1.24.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py->allennlp) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboardX>=1.2->allennlp) (3.8.0)\n",
      "Requirement already satisfied: html5lib in c:\\programdata\\anaconda3\\lib\\site-packages (from ftfy->allennlp) (1.0.1)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from ftfy->allennlp) (0.1.7)\n",
      "Requirement already satisfied: greenlet>=0.4.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: cffi>=1.11.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from gevent>=1.3.6->allennlp) (1.12.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=1.0.2->allennlp) (0.14.1)\n",
      "Requirement already satisfied: click>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from flask>=1.0.2->allennlp) (2.10)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-transformers==1.1.0->allennlp) (2017.4.5)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-transformers==1.1.0->allennlp) (0.1.83)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->allennlp) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->allennlp) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->allennlp) (2.8.0)\n",
      "Requirement already satisfied: sphinx>=1.2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
      "Requirement already satisfied: singledispatch in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk->allennlp) (3.4.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.2)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.28.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy<2.2,>=2.1.0->allennlp) (0.0.7)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (1.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omri\\appdata\\roaming\\python\\python37\\site-packages (from pytest->allennlp) (41.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (0.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest->allennlp) (0.4.1)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->allennlp) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.110 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->allennlp) (1.12.234)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->allennlp) (0.9.4)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from html5lib->ftfy->allennlp) (0.5.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.11.5->gevent>=1.3.6->allennlp) (2.19)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=2.10->flask>=1.0.2->allennlp) (1.1.1)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.3.1)\n",
      "Requirement already satisfied: docutils>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.14)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.2.1)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (2.6.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: imagesize in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (19.0)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in c:\\programdata\\anaconda3\\lib\\site-packages (from sphinx>=1.2.3->numpydoc>=0.8.0->allennlp) (1.1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in c:\\programdata\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: mpld3==0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (2017.4.5)\n",
      "Requirement already satisfied: langdetect in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.0.7)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: pytest>=3.6.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (4.3.1)\n",
      "Requirement already satisfied: torch>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.3.0+cpu)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.2)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.0)\n",
      "Requirement already satisfied: tabulate in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.8.5)\n",
      "Requirement already satisfied: pytorch-transformers>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (4.31.1)\n",
      "Requirement already satisfied: ipython==7.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (7.6.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.5.7)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.24.1)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (1.2.6)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (3.0.3)\n",
      "Requirement already satisfied: gensim>=3.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (3.7.1)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from langdetect->flair) (1.12.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from bpemb>=0.2.9->flair) (0.1.83)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from bpemb>=0.2.9->flair) (1.16.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from bpemb>=0.2.9->flair) (2.21.0)\n",
      "Requirement already satisfied: py>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omri\\appdata\\roaming\\python\\python37\\site-packages (from pytest>=3.6.4->flair) (41.0.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: pluggy>=0.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (0.9.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (6.0.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from pytest>=3.6.4->flair) (0.4.1)\n",
      "Requirement already satisfied: bson in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (0.5.8)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (1.2.1)\n",
      "Requirement already satisfied: networkx==2.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (2.2)\n",
      "Requirement already satisfied: cloudpickle in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (0.8.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from hyperopt>=0.1.1->flair) (0.17.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn->flair) (0.20.3)\n",
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pytorch-transformers>=1.1.0->flair) (1.9.110)\n",
      "Requirement already satisfied: pickleshare in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (2.0.9)\n",
      "Requirement already satisfied: backcall in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (4.3.2)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (0.13.3)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (4.4.0)\n",
      "Requirement already satisfied: pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython==7.6.1->flair) (2.3.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib>=2.2.3->flair) (2.8.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim>=3.4.0->flair) (1.8.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.2.9->flair) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->bpemb>=0.2.9->flair) (2.8)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.110 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->flair) (1.12.234)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->flair) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from boto3->pytorch-transformers>=1.1.0->flair) (0.9.4)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython==7.6.1->flair) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.3.4)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim>=3.4.0->flair) (2.49.0)\n",
      "Requirement already satisfied: bz2file in c:\\programdata\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim>=3.4.0->flair) (0.98)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.110->boto3->pytorch-transformers>=1.1.0->flair) (0.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install allennlp\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "sentence = Sentence('The grass is green .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also init a class of the desired embedding method. The `embed` method of this class gets a Sentence and adds to its tokens the relevant embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "torch.Size([3072])\n",
      "tensor([-0.3288,  0.2022, -0.5940,  ..., -1.2773,  0.3049,  0.2150])\n",
      "Token: 2 grass\n",
      "torch.Size([3072])\n",
      "tensor([ 0.2539, -0.2363,  0.5263,  ..., -0.7001,  0.8798,  1.4191])\n",
      "Token: 3 is\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1915,  0.2300, -0.2894,  ..., -0.3626,  1.9066,  1.4520])\n",
      "Token: 4 green\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1779,  0.1309, -0.1041,  ..., -0.1006,  1.6152,  0.3299])\n",
      "Token: 5 .\n",
      "torch.Size([3072])\n",
      "tensor([-0.8872, -0.2004, -1.0601,  ..., -0.0106, -0.0833,  0.0669])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "elmo_embedding = ELMoEmbeddings()\n",
    "\n",
    "elmo_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Now, compare the embeddings obtained using ELMo for the same word in different contexts. Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word sense disambiguation using ELMo\n",
    "**Try it yourself:** Let's also try to see how ELMo handles word sense disambiguation. Below are 6 sentences with 2 different meanings of the word `bank`. Try to see if ELMo vectors indeed separate the two meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I was walking along the river bank\",\n",
    "    \"I saw a toad near the east bank of the river\",\n",
    "    \"We had a nice picnic by the bank\",\n",
    "    \"I need to deposit money from the bank\",\n",
    "    \"The bank branch is closed\",\n",
    "    \"He started working at the bank\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents using the average of contextual word vectors\n",
    "**Try it yourself:** *Optional:* In previous sections we've built a classifier using the average of non-contextual word vectors. Now, try to use contextual word embeddings on our dataset. Use the average of these vectors and apply a classifier on it to obtain the predictions. Is the performance better than for non-contextual word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embedding using ELMo\n",
    "We've used Flair to get embeddings for each word in the sentence. However, for text classification of the entire document, we need a way to integrate all these vectors into a single document embedding. There are several methods for that, and those interested would find this article useful - https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d\n",
    "\n",
    "The most basic element is averaging the word embedding into a single document embedding. In FLAIR, we do this using a DocumentPooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DocumentPoolEmbeddings(\n",
       "  fine_tune_mode=linear, pooling=mean\n",
       "  (embeddings): StackedEmbeddings(\n",
       "    (list_embedding_0): ELMoEmbeddings(model=elmo-original)\n",
       "  )\n",
       "  (embedding_flex): Linear(in_features=3072, out_features=3072, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([elmo_embedding], pooling='mean')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "tensor([-0.1185,  0.0253, -0.3043,  ..., -0.4902,  0.9246,  0.6966],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding().shape)\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use an RNN that runs over the word embeddings. We will use the last hidden state as the document embedding. In this case it is very helpful to train the model using the true labels of our task, so that the RNN is optimized for our own data and task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "document_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1027, -0.1833,  0.1588, -0.0189, -0.0200, -0.0450,  0.0138,  0.0329,\n",
      "        -0.1762,  0.0325,  0.2245, -0.0553, -0.0456, -0.0457,  0.3575,  0.0486,\n",
      "         0.3647, -0.0574, -0.1374, -0.0072,  0.1516, -0.0705, -0.1483, -0.0227,\n",
      "         0.0541, -0.0501, -0.0200, -0.3354,  0.1682,  0.0555, -0.2173, -0.0564,\n",
      "        -0.0606,  0.0550,  0.1950,  0.0486, -0.0994,  0.1542,  0.0731, -0.0298,\n",
      "         0.0631,  0.1804,  0.0638, -0.0166, -0.0665, -0.1210, -0.2998, -0.1263,\n",
      "        -0.2099, -0.1629,  0.2091, -0.1292,  0.1359, -0.0130,  0.1932,  0.0120,\n",
      "        -0.0294,  0.3837,  0.1122, -0.0678, -0.0818,  0.1508, -0.0017, -0.0788,\n",
      "         0.0623,  0.0702,  0.2679, -0.0483, -0.0035, -0.1998,  0.0654, -0.2028,\n",
      "        -0.1047,  0.1110,  0.1318, -0.2363,  0.0176, -0.0227,  0.0753,  0.0144,\n",
      "        -0.0770, -0.0076, -0.0836, -0.1628,  0.0625,  0.0755, -0.1855,  0.0374,\n",
      "         0.0665,  0.0162,  0.0702, -0.0649, -0.0161, -0.0772,  0.3199, -0.1396,\n",
      "         0.1110,  0.0626, -0.0382,  0.1907,  0.1822,  0.0696,  0.0628, -0.0128,\n",
      "         0.0840,  0.0316, -0.0243, -0.1160,  0.0997,  0.0120,  0.0314,  0.0819,\n",
      "        -0.0309,  0.0122, -0.0516,  0.1233,  0.0731, -0.0403,  0.1179, -0.1060,\n",
      "         0.1027,  0.0941, -0.1596, -0.0926, -0.1667,  0.0724,  0.1324,  0.1192],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while `DocumentPoolEmbeddings` are immediately meaningful, `DocumentRNNEmbeddings` need to be tuned on the downstream task. This happens automatically in Flair if you train a new model with these embeddings. You can find an example of training a text classification model [here](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model). Once the model is trained, you can access the tuned DocumentRNNEmbeddings object directly from the classifier object and use it to embed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DocumentRNNEmbeddings` have a number of hyper-parameters that can be tuned to improve learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ":param hidden_size: the number of hidden states in the rnn.\n",
    ":param rnn_layers: the number of layers for the rnn.\n",
    ":param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
    "layer before putting them into the rnn or not.\n",
    ":param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
    "dimension as before will be taken.\n",
    ":param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not.\n",
    ":param dropout: the dropout value to be used.\n",
    ":param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n",
    ":param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n",
    ":param rnn_type: one of 'RNN' or 'LSTM'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "The simplest way to load our data in Flair is using a CSV file. You can learn about other method in [the documentation](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Corpus` for a text classification task, you need to have three files (train, dev, and test) in the \n",
    "above format located in one folder. This data folder structure could, for example, look like this for the IMDB task:\n",
    "```text\n",
    "/data/train.csv\n",
    "/data/val.csv\n",
    "/data/test.txt\n",
    "```\n",
    "Now create a `CSVClassificationCorpus` by pointing to this folder (`/data`). \n",
    "Thereby, each line in a file is converted to a `Sentence` object annotated with the labels.\n",
    "\n",
    "Attention: A text in a line can in fact have multiple sentences. Thus, a `Sentence` object is actually a `Document` and can actually consist of multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25 06:39:43,110 Reading data from data\n",
      "2019-10-25 06:39:43,111 Train: data\\train.csv\n",
      "2019-10-25 06:39:43,112 Dev: data\\val.csv\n",
      "2019-10-25 06:39:43,112 Test: data\\test.csv\n",
      "2019-10-25 06:39:43,125 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 764/764 [00:00<00:00, 1152.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25 06:39:58,081 [b'talk.politics.guns', b'rec.sport.baseball', b'rec.sport.hockey', b'talk.politics.mideast']\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'data/'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s). This is 1-based and not 0-based\n",
    "column_name_map = {5: \"text\", 4: \"label\"}\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                      test_file='test.csv',\n",
    "                                      dev_file='val.csv',\n",
    "                                      train_file='train.csv')\n",
    "    \n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence: \"check again . you may find that the arrest warrant was issued after the first firefight . --\" - 18 Tokens"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our own model\n",
    "We're going to use an RNN to run through the contextual word embeddings we got from ELMo. We will use the hidden state at the end of the document as an embedding for the entire document. We will train the RNN on our labeled dataset, so that the final hidden state carries the most relevant information for our custom classification task.  \n",
    "\n",
    "For more information on training your own model using Flair, see [this tutorial](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change code below to fit our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [WordEmbeddings('glove')]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25 06:40:32,949 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,950 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=4, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-10-25 06:40:32,951 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,952 Corpus: \"Corpus: 764 train + 164 dev + 164 test sentences\"\n",
      "2019-10-25 06:40:32,953 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,954 Parameters:\n",
      "2019-10-25 06:40:32,955  - learning_rate: \"0.1\"\n",
      "2019-10-25 06:40:32,956  - mini_batch_size: \"32\"\n",
      "2019-10-25 06:40:32,956  - patience: \"5\"\n",
      "2019-10-25 06:40:32,957  - anneal_factor: \"0.5\"\n",
      "2019-10-25 06:40:32,958  - max_epochs: \"150\"\n",
      "2019-10-25 06:40:32,959  - shuffle: \"True\"\n",
      "2019-10-25 06:40:32,960  - train_with_dev: \"False\"\n",
      "2019-10-25 06:40:32,961 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,962 Model training base path: \"data\"\n",
      "2019-10-25 06:40:32,963 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,963 Device: cpu\n",
      "2019-10-25 06:40:32,964 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:32,965 Embeddings storage mode: cpu\n",
      "2019-10-25 06:40:32,967 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:40:44,349 epoch 1 - iter 0/24 - loss 1.49370718 - samples/sec: 87.02\n",
      "2019-10-25 06:40:45,895 epoch 1 - iter 2/24 - loss 1.66083467 - samples/sec: 42.77\n",
      "2019-10-25 06:40:47,505 epoch 1 - iter 4/24 - loss 1.57539825 - samples/sec: 40.93\n",
      "2019-10-25 06:40:48,974 epoch 1 - iter 6/24 - loss 1.52626928 - samples/sec: 44.97\n",
      "2019-10-25 06:40:50,463 epoch 1 - iter 8/24 - loss 1.52883701 - samples/sec: 44.50\n",
      "2019-10-25 06:40:51,812 epoch 1 - iter 10/24 - loss 1.50611903 - samples/sec: 48.93\n",
      "2019-10-25 06:40:53,449 epoch 1 - iter 12/24 - loss 1.48764301 - samples/sec: 41.37\n",
      "2019-10-25 06:40:55,190 epoch 1 - iter 14/24 - loss 1.48706020 - samples/sec: 37.96\n",
      "2019-10-25 06:40:57,000 epoch 1 - iter 16/24 - loss 1.47729754 - samples/sec: 36.46\n",
      "2019-10-25 06:40:58,726 epoch 1 - iter 18/24 - loss 1.46302881 - samples/sec: 38.27\n",
      "2019-10-25 06:41:00,594 epoch 1 - iter 20/24 - loss 1.45581440 - samples/sec: 35.39\n",
      "2019-10-25 06:41:02,386 epoch 1 - iter 22/24 - loss 1.45660043 - samples/sec: 36.70\n",
      "2019-10-25 06:41:03,551 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:41:03,553 EPOCH 1 done: loss 1.4516 - lr 0.1000\n",
      "2019-10-25 06:41:14,998 DEV : loss 1.3400391340255737 - score 0.2744\n",
      "2019-10-25 06:41:15,114 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25 06:41:18,046 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:41:29,540 epoch 2 - iter 0/24 - loss 1.38134038 - samples/sec: 89.03\n",
      "2019-10-25 06:41:31,434 epoch 2 - iter 2/24 - loss 1.27515229 - samples/sec: 34.80\n",
      "2019-10-25 06:41:33,158 epoch 2 - iter 4/24 - loss 1.24434588 - samples/sec: 38.15\n",
      "2019-10-25 06:41:35,326 epoch 2 - iter 6/24 - loss 1.28255429 - samples/sec: 30.41\n",
      "2019-10-25 06:41:37,470 epoch 2 - iter 8/24 - loss 1.29598440 - samples/sec: 30.82\n",
      "2019-10-25 06:41:39,173 epoch 2 - iter 10/24 - loss 1.31206984 - samples/sec: 39.19\n",
      "2019-10-25 06:41:40,820 epoch 2 - iter 12/24 - loss 1.30308888 - samples/sec: 40.35\n",
      "2019-10-25 06:41:43,195 epoch 2 - iter 14/24 - loss 1.30053051 - samples/sec: 33.11\n",
      "2019-10-25 06:41:45,092 epoch 2 - iter 16/24 - loss 1.29916115 - samples/sec: 34.79\n",
      "2019-10-25 06:41:47,098 epoch 2 - iter 18/24 - loss 1.29539271 - samples/sec: 32.96\n",
      "2019-10-25 06:41:48,824 epoch 2 - iter 20/24 - loss 1.29464445 - samples/sec: 38.32\n",
      "2019-10-25 06:41:50,599 epoch 2 - iter 22/24 - loss 1.29447735 - samples/sec: 37.17\n",
      "2019-10-25 06:41:52,076 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:41:52,077 EPOCH 2 done: loss 1.2946 - lr 0.1000\n",
      "2019-10-25 06:42:05,811 DEV : loss 1.3258041143417358 - score 0.3293\n",
      "2019-10-25 06:42:05,939 BAD EPOCHS (no improvement): 0\n",
      "2019-10-25 06:42:08,756 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:42:20,625 epoch 3 - iter 0/24 - loss 1.46180856 - samples/sec: 74.88\n",
      "2019-10-25 06:42:22,620 epoch 3 - iter 2/24 - loss 1.29437613 - samples/sec: 33.08\n",
      "2019-10-25 06:42:24,567 epoch 3 - iter 4/24 - loss 1.23508685 - samples/sec: 33.55\n",
      "2019-10-25 06:42:26,447 epoch 3 - iter 6/24 - loss 1.23336143 - samples/sec: 35.26\n",
      "2019-10-25 06:42:28,071 epoch 3 - iter 8/24 - loss 1.25866107 - samples/sec: 40.89\n",
      "2019-10-25 06:42:29,820 epoch 3 - iter 10/24 - loss 1.28187623 - samples/sec: 37.45\n",
      "2019-10-25 06:42:31,653 epoch 3 - iter 12/24 - loss 1.27474672 - samples/sec: 36.10\n",
      "2019-10-25 06:42:33,425 epoch 3 - iter 14/24 - loss 1.27405032 - samples/sec: 37.37\n",
      "2019-10-25 06:42:35,250 epoch 3 - iter 16/24 - loss 1.27495852 - samples/sec: 35.97\n",
      "2019-10-25 06:42:37,193 epoch 3 - iter 18/24 - loss 1.27672600 - samples/sec: 34.17\n",
      "2019-10-25 06:42:39,047 epoch 3 - iter 20/24 - loss 1.27977267 - samples/sec: 35.60\n",
      "2019-10-25 06:42:40,726 epoch 3 - iter 22/24 - loss 1.27488453 - samples/sec: 39.14\n",
      "2019-10-25 06:42:41,703 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:42:41,705 EPOCH 3 done: loss 1.2706 - lr 0.1000\n",
      "2019-10-25 06:42:54,407 DEV : loss 1.2319387197494507 - score 0.3963\n",
      "2019-10-25 06:42:54,536 BAD EPOCHS (no improvement): 0\n",
      "2019-10-25 06:42:57,483 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:43:09,139 epoch 4 - iter 0/24 - loss 1.24534404 - samples/sec: 79.82\n",
      "2019-10-25 06:43:10,704 epoch 4 - iter 2/24 - loss 1.28002946 - samples/sec: 41.98\n",
      "2019-10-25 06:43:12,315 epoch 4 - iter 4/24 - loss 1.20169058 - samples/sec: 40.97\n",
      "2019-10-25 06:43:14,026 epoch 4 - iter 6/24 - loss 1.21453953 - samples/sec: 38.82\n",
      "2019-10-25 06:43:16,001 epoch 4 - iter 8/24 - loss 1.23723184 - samples/sec: 33.32\n",
      "2019-10-25 06:43:17,761 epoch 4 - iter 10/24 - loss 1.24058857 - samples/sec: 37.47\n",
      "2019-10-25 06:43:19,718 epoch 4 - iter 12/24 - loss 1.23876667 - samples/sec: 43.94\n",
      "2019-10-25 06:43:21,561 epoch 4 - iter 14/24 - loss 1.22932731 - samples/sec: 35.68\n",
      "2019-10-25 06:43:23,397 epoch 4 - iter 16/24 - loss 1.22822231 - samples/sec: 35.77\n",
      "2019-10-25 06:43:25,433 epoch 4 - iter 18/24 - loss 1.22904843 - samples/sec: 32.59\n",
      "2019-10-25 06:43:27,281 epoch 4 - iter 20/24 - loss 1.23261417 - samples/sec: 35.67\n",
      "2019-10-25 06:43:29,209 epoch 4 - iter 22/24 - loss 1.22847642 - samples/sec: 34.06\n",
      "2019-10-25 06:43:30,593 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:43:30,595 EPOCH 4 done: loss 1.2284 - lr 0.1000\n",
      "2019-10-25 06:43:52,040 DEV : loss 1.2219551801681519 - score 0.3841\n",
      "2019-10-25 06:43:52,213 BAD EPOCHS (no improvement): 1\n",
      "2019-10-25 06:43:52,216 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:44:06,991 epoch 5 - iter 0/24 - loss 1.02426541 - samples/sec: 72.43\n",
      "2019-10-25 06:44:08,961 epoch 5 - iter 2/24 - loss 1.12774396 - samples/sec: 33.69\n",
      "2019-10-25 06:44:11,018 epoch 5 - iter 4/24 - loss 1.17516201 - samples/sec: 31.77\n",
      "2019-10-25 06:44:13,176 epoch 5 - iter 6/24 - loss 1.19186359 - samples/sec: 30.50\n",
      "2019-10-25 06:44:15,239 epoch 5 - iter 8/24 - loss 1.19857278 - samples/sec: 32.14\n",
      "2019-10-25 06:44:17,338 epoch 5 - iter 10/24 - loss 1.17900890 - samples/sec: 31.19\n",
      "2019-10-25 06:44:19,044 epoch 5 - iter 12/24 - loss 1.19842371 - samples/sec: 38.82\n",
      "2019-10-25 06:44:20,971 epoch 5 - iter 14/24 - loss 1.19124865 - samples/sec: 34.38\n",
      "2019-10-25 06:44:22,761 epoch 5 - iter 16/24 - loss 1.19321835 - samples/sec: 37.67\n",
      "2019-10-25 06:44:24,744 epoch 5 - iter 18/24 - loss 1.17937747 - samples/sec: 33.18\n",
      "2019-10-25 06:44:26,633 epoch 5 - iter 20/24 - loss 1.17501188 - samples/sec: 35.10\n",
      "2019-10-25 06:44:28,461 epoch 5 - iter 22/24 - loss 1.17509473 - samples/sec: 35.86\n",
      "2019-10-25 06:44:29,524 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:44:29,526 EPOCH 5 done: loss 1.1777 - lr 0.1000\n",
      "2019-10-25 06:44:44,252 DEV : loss 1.2259411811828613 - score 0.3841\n",
      "2019-10-25 06:44:44,377 BAD EPOCHS (no improvement): 2\n",
      "2019-10-25 06:44:44,379 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:45:01,237 epoch 6 - iter 0/24 - loss 1.02712762 - samples/sec: 67.16\n",
      "2019-10-25 06:45:02,957 epoch 6 - iter 2/24 - loss 1.13114409 - samples/sec: 38.51\n",
      "2019-10-25 06:45:04,674 epoch 6 - iter 4/24 - loss 1.12224784 - samples/sec: 38.34\n",
      "2019-10-25 06:45:06,507 epoch 6 - iter 6/24 - loss 1.15079357 - samples/sec: 36.17\n",
      "2019-10-25 06:45:08,455 epoch 6 - iter 8/24 - loss 1.16654707 - samples/sec: 33.79\n",
      "2019-10-25 06:45:10,618 epoch 6 - iter 10/24 - loss 1.19052150 - samples/sec: 30.25\n",
      "2019-10-25 06:45:12,178 epoch 6 - iter 12/24 - loss 1.19376979 - samples/sec: 42.52\n",
      "2019-10-25 06:45:14,081 epoch 6 - iter 14/24 - loss 1.18260033 - samples/sec: 34.68\n",
      "2019-10-25 06:45:16,115 epoch 6 - iter 16/24 - loss 1.17225082 - samples/sec: 32.28\n",
      "2019-10-25 06:45:17,947 epoch 6 - iter 18/24 - loss 1.16228885 - samples/sec: 36.36\n",
      "2019-10-25 06:45:19,641 epoch 6 - iter 20/24 - loss 1.15199870 - samples/sec: 39.04\n",
      "2019-10-25 06:45:21,489 epoch 6 - iter 22/24 - loss 1.14879642 - samples/sec: 35.50\n",
      "2019-10-25 06:45:22,553 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:45:22,554 EPOCH 6 done: loss 1.1540 - lr 0.1000\n",
      "2019-10-25 06:45:34,150 DEV : loss 1.3024336099624634 - score 0.372\n",
      "2019-10-25 06:45:34,277 BAD EPOCHS (no improvement): 3\n",
      "2019-10-25 06:45:34,279 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:45:45,937 epoch 7 - iter 0/24 - loss 1.26932406 - samples/sec: 67.11\n",
      "2019-10-25 06:45:47,643 epoch 7 - iter 2/24 - loss 1.25166504 - samples/sec: 38.84\n",
      "2019-10-25 06:45:49,425 epoch 7 - iter 4/24 - loss 1.25595636 - samples/sec: 37.00\n",
      "2019-10-25 06:45:51,269 epoch 7 - iter 6/24 - loss 1.29176666 - samples/sec: 35.99\n",
      "2019-10-25 06:45:53,159 epoch 7 - iter 8/24 - loss 1.33106335 - samples/sec: 34.77\n",
      "2019-10-25 06:45:54,822 epoch 7 - iter 10/24 - loss 1.30735317 - samples/sec: 39.71\n",
      "2019-10-25 06:45:56,624 epoch 7 - iter 12/24 - loss 1.26918506 - samples/sec: 36.89\n",
      "2019-10-25 06:45:58,213 epoch 7 - iter 14/24 - loss 1.25075198 - samples/sec: 41.41\n",
      "2019-10-25 06:45:59,914 epoch 7 - iter 16/24 - loss 1.23510830 - samples/sec: 38.76\n",
      "2019-10-25 06:46:01,997 epoch 7 - iter 18/24 - loss 1.22254636 - samples/sec: 31.78\n",
      "2019-10-25 06:46:03,638 epoch 7 - iter 20/24 - loss 1.20409148 - samples/sec: 40.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25 06:46:05,578 epoch 7 - iter 22/24 - loss 1.19991632 - samples/sec: 33.96\n",
      "2019-10-25 06:46:06,903 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:46:06,905 EPOCH 7 done: loss 1.1955 - lr 0.1000\n",
      "2019-10-25 06:46:18,289 DEV : loss 1.2214267253875732 - score 0.3537\n",
      "2019-10-25 06:46:18,397 BAD EPOCHS (no improvement): 4\n",
      "2019-10-25 06:46:18,400 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:46:32,221 epoch 8 - iter 0/24 - loss 1.11569440 - samples/sec: 86.55\n",
      "2019-10-25 06:46:33,998 epoch 8 - iter 2/24 - loss 1.07375638 - samples/sec: 37.03\n",
      "2019-10-25 06:46:35,886 epoch 8 - iter 4/24 - loss 1.16339598 - samples/sec: 34.99\n",
      "2019-10-25 06:46:37,751 epoch 8 - iter 6/24 - loss 1.19701326 - samples/sec: 35.62\n",
      "2019-10-25 06:46:39,480 epoch 8 - iter 8/24 - loss 1.17006695 - samples/sec: 38.20\n",
      "2019-10-25 06:46:41,424 epoch 8 - iter 10/24 - loss 1.15266242 - samples/sec: 33.78\n",
      "2019-10-25 06:46:43,604 epoch 8 - iter 12/24 - loss 1.16087550 - samples/sec: 38.56\n",
      "2019-10-25 06:46:45,400 epoch 8 - iter 14/24 - loss 1.16384623 - samples/sec: 36.76\n",
      "2019-10-25 06:46:47,341 epoch 8 - iter 16/24 - loss 1.16130214 - samples/sec: 34.33\n",
      "2019-10-25 06:46:49,071 epoch 8 - iter 18/24 - loss 1.14735534 - samples/sec: 38.32\n",
      "2019-10-25 06:46:50,984 epoch 8 - iter 20/24 - loss 1.14257476 - samples/sec: 34.41\n",
      "2019-10-25 06:46:52,764 epoch 8 - iter 22/24 - loss 1.14323298 - samples/sec: 36.98\n",
      "2019-10-25 06:46:53,863 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:46:53,865 EPOCH 8 done: loss 1.1492 - lr 0.1000\n",
      "2019-10-25 06:47:05,614 DEV : loss 1.2085317373275757 - score 0.4268\n",
      "2019-10-25 06:47:05,727 BAD EPOCHS (no improvement): 0\n",
      "2019-10-25 06:47:08,537 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:47:20,110 epoch 9 - iter 0/24 - loss 0.97680891 - samples/sec: 74.93\n",
      "2019-10-25 06:47:21,823 epoch 9 - iter 2/24 - loss 1.00169255 - samples/sec: 38.60\n",
      "2019-10-25 06:47:23,681 epoch 9 - iter 4/24 - loss 1.08606230 - samples/sec: 35.36\n",
      "2019-10-25 06:47:25,858 epoch 9 - iter 6/24 - loss 1.11664546 - samples/sec: 30.37\n",
      "2019-10-25 06:47:27,840 epoch 9 - iter 8/24 - loss 1.08694990 - samples/sec: 33.45\n",
      "2019-10-25 06:47:29,509 epoch 9 - iter 10/24 - loss 1.09188288 - samples/sec: 39.56\n",
      "2019-10-25 06:47:31,356 epoch 9 - iter 12/24 - loss 1.09346728 - samples/sec: 35.84\n",
      "2019-10-25 06:47:33,263 epoch 9 - iter 14/24 - loss 1.11769158 - samples/sec: 34.50\n",
      "2019-10-25 06:47:35,267 epoch 9 - iter 16/24 - loss 1.16920258 - samples/sec: 32.78\n",
      "2019-10-25 06:47:37,274 epoch 9 - iter 18/24 - loss 1.15600981 - samples/sec: 32.98\n",
      "2019-10-25 06:47:39,217 epoch 9 - iter 20/24 - loss 1.14515976 - samples/sec: 33.95\n",
      "2019-10-25 06:47:41,240 epoch 9 - iter 22/24 - loss 1.15424331 - samples/sec: 32.46\n",
      "2019-10-25 06:47:43,078 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:47:43,080 EPOCH 9 done: loss 1.1557 - lr 0.1000\n",
      "2019-10-25 06:47:56,704 DEV : loss 1.205046534538269 - score 0.3963\n",
      "2019-10-25 06:47:56,826 BAD EPOCHS (no improvement): 1\n",
      "2019-10-25 06:47:56,828 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:48:09,661 epoch 10 - iter 0/24 - loss 1.12677407 - samples/sec: 79.96\n",
      "2019-10-25 06:48:11,624 epoch 10 - iter 2/24 - loss 1.03232638 - samples/sec: 33.78\n",
      "2019-10-25 06:48:13,376 epoch 10 - iter 4/24 - loss 1.07426832 - samples/sec: 37.51\n",
      "2019-10-25 06:48:15,031 epoch 10 - iter 6/24 - loss 1.08873587 - samples/sec: 40.25\n",
      "2019-10-25 06:48:16,944 epoch 10 - iter 8/24 - loss 1.09692098 - samples/sec: 34.65\n",
      "2019-10-25 06:48:18,654 epoch 10 - iter 10/24 - loss 1.09070301 - samples/sec: 38.35\n",
      "2019-10-25 06:48:20,741 epoch 10 - iter 12/24 - loss 1.07562200 - samples/sec: 40.18\n",
      "2019-10-25 06:48:22,601 epoch 10 - iter 14/24 - loss 1.06614679 - samples/sec: 35.57\n",
      "2019-10-25 06:48:24,296 epoch 10 - iter 16/24 - loss 1.07104227 - samples/sec: 38.72\n",
      "2019-10-25 06:48:26,266 epoch 10 - iter 18/24 - loss 1.08764726 - samples/sec: 33.56\n",
      "2019-10-25 06:48:28,117 epoch 10 - iter 20/24 - loss 1.08230280 - samples/sec: 35.86\n",
      "2019-10-25 06:48:29,816 epoch 10 - iter 22/24 - loss 1.07468453 - samples/sec: 38.67\n",
      "2019-10-25 06:48:30,867 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:48:30,868 EPOCH 10 done: loss 1.0638 - lr 0.1000\n",
      "2019-10-25 06:48:42,311 DEV : loss 1.1346577405929565 - score 0.4207\n",
      "2019-10-25 06:48:42,432 BAD EPOCHS (no improvement): 2\n",
      "2019-10-25 06:48:42,434 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:48:53,952 epoch 11 - iter 0/24 - loss 1.01405311 - samples/sec: 75.85\n",
      "2019-10-25 06:48:55,712 epoch 11 - iter 2/24 - loss 0.96888596 - samples/sec: 37.58\n",
      "2019-10-25 06:48:57,467 epoch 11 - iter 4/24 - loss 0.97660027 - samples/sec: 37.78\n",
      "2019-10-25 06:48:59,190 epoch 11 - iter 6/24 - loss 1.06559675 - samples/sec: 38.74\n",
      "2019-10-25 06:49:00,856 epoch 11 - iter 8/24 - loss 1.07065632 - samples/sec: 39.62\n",
      "2019-10-25 06:49:02,700 epoch 11 - iter 10/24 - loss 1.06973840 - samples/sec: 35.71\n",
      "2019-10-25 06:49:04,644 epoch 11 - iter 12/24 - loss 1.06038025 - samples/sec: 34.25\n",
      "2019-10-25 06:49:06,377 epoch 11 - iter 14/24 - loss 1.07752624 - samples/sec: 38.02\n",
      "2019-10-25 06:49:08,208 epoch 11 - iter 16/24 - loss 1.07876001 - samples/sec: 35.95\n",
      "2019-10-25 06:49:10,062 epoch 11 - iter 18/24 - loss 1.08786936 - samples/sec: 36.07\n",
      "2019-10-25 06:49:11,813 epoch 11 - iter 20/24 - loss 1.09268912 - samples/sec: 37.64\n",
      "2019-10-25 06:49:13,772 epoch 11 - iter 22/24 - loss 1.08822932 - samples/sec: 33.62\n",
      "2019-10-25 06:49:15,053 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:49:15,054 EPOCH 11 done: loss 1.0891 - lr 0.1000\n",
      "2019-10-25 06:49:26,707 DEV : loss 1.1110105514526367 - score 0.4695\n",
      "2019-10-25 06:49:26,827 BAD EPOCHS (no improvement): 0\n",
      "2019-10-25 06:49:29,481 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:49:41,256 epoch 12 - iter 0/24 - loss 1.04942822 - samples/sec: 62.24\n",
      "2019-10-25 06:49:42,953 epoch 12 - iter 2/24 - loss 1.28411837 - samples/sec: 39.02\n",
      "2019-10-25 06:49:44,754 epoch 12 - iter 4/24 - loss 1.19519532 - samples/sec: 36.50\n",
      "2019-10-25 06:49:46,722 epoch 12 - iter 6/24 - loss 1.14635070 - samples/sec: 33.76\n",
      "2019-10-25 06:49:48,712 epoch 12 - iter 8/24 - loss 1.09940680 - samples/sec: 33.15\n",
      "2019-10-25 06:49:50,575 epoch 12 - iter 10/24 - loss 1.11606169 - samples/sec: 35.20\n",
      "2019-10-25 06:49:52,900 epoch 12 - iter 12/24 - loss 1.13330933 - samples/sec: 35.17\n",
      "2019-10-25 06:49:54,849 epoch 12 - iter 14/24 - loss 1.13795190 - samples/sec: 33.89\n",
      "2019-10-25 06:49:57,159 epoch 12 - iter 16/24 - loss 1.12340417 - samples/sec: 28.36\n",
      "2019-10-25 06:49:59,074 epoch 12 - iter 18/24 - loss 1.09578918 - samples/sec: 34.73\n",
      "2019-10-25 06:50:00,990 epoch 12 - iter 20/24 - loss 1.09357917 - samples/sec: 34.45\n",
      "2019-10-25 06:50:02,930 epoch 12 - iter 22/24 - loss 1.08048112 - samples/sec: 33.77\n",
      "2019-10-25 06:50:04,355 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:50:04,357 EPOCH 12 done: loss 1.0887 - lr 0.1000\n",
      "2019-10-25 06:50:17,028 DEV : loss 1.6641587018966675 - score 0.3232\n",
      "2019-10-25 06:50:17,144 BAD EPOCHS (no improvement): 1\n",
      "2019-10-25 06:50:17,147 ----------------------------------------------------------------------------------------------------\n",
      "2019-10-25 06:50:30,681 epoch 13 - iter 0/24 - loss 1.24824691 - samples/sec: 75.84\n",
      "2019-10-25 06:50:32,437 epoch 13 - iter 2/24 - loss 1.02547894 - samples/sec: 37.61\n"
     ]
    }
   ],
   "source": [
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('data/',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. plot weight traces (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('data/weights.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Word Vectors with BERT and Stacking Embeddings\n",
    "We will later use BERT, a state-of-the-art transformer model that was trained on a very large corpus and can be fine-tuned for our own custom task. The Flair package can also be used to derive contextual word embeddings using BERT and its successors. We will use a different package for BERT, to provide you with sample code for using it, and for adaptation of the weights of the BERT model itself for our own task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can try to use BERT, Roberta, XLNet or other models provided in Flair for contextual word embeddings.  \n",
    "Flair also provides a simple way to stack vectors from different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:38:42,789 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt not found in cache, downloading to C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 73034300/73034300 [00:08<00:00, 8530759.25B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:38:51,700 copying C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb to cache at C:\\Users\\Omri\\.flair\\embeddings\\lm-multi-forward-v0.1.pt\n",
      "2019-10-17 08:38:51,779 removing temp file C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb\n",
      "2019-10-17 08:38:52,276 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt not found in cache, downloading to C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 73034304/73034304 [00:12<00:00, 5879942.68B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:39:05,094 copying C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp to cache at C:\\Users\\Omri\\.flair\\embeddings\\lm-multi-backward-v0.1.pt\n",
      "2019-10-17 08:39:05,172 removing temp file C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 995526/995526 [00:00<00:00, 1143581.85B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 521/521 [00:00<00:00, 262364.32B/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 714314041/714314041 [01:08<00:00, 10488502.38B/s]\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings\n",
    "\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-1.4812e-07,  4.5007e-08,  6.0273e-07,  ...,  3.8287e-01,\n",
      "         4.7210e-01,  2.9850e-01])\n",
      "Token: 2 grass\n",
      "tensor([ 1.6254e-04,  1.8764e-07, -7.9038e-09,  ...,  8.5283e-01,\n",
      "        -5.0726e-02,  3.4476e-01])\n",
      "Token: 3 is\n",
      "tensor([-2.4521e-04,  3.4869e-07,  5.5841e-06,  ..., -1.8283e-01,\n",
      "         7.1532e-01,  5.0841e-03])\n",
      "Token: 4 green\n",
      "tensor([8.3005e-05, 4.7261e-08, 5.7315e-07,  ..., 1.0157e+00, 7.5358e-01,\n",
      "        1.1230e-01])\n",
      "Token: 5 .\n",
      "tensor([-8.3244e-07,  1.6451e-07, -1.7201e-08,  ..., -6.0930e-01,\n",
      "         9.0591e-01,  1.7857e-01])\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Train a classifier using stacked embeddings of different models. Do you see an increase in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
