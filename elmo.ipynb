{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo\n",
    "ELMo learns contextualized word vectors by running the text through a deep recurrent network.  \n",
    "ELMo is actually an algorithm for unsupervised learning and does not make any use of the labels we have for our text classification task. The authors do show that contextualized word vectors obtained using ELMo increase text classification performance in a large array of tasks. Let's see if we see a significant gain in our case!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are good examples of using ELMo in both [the AllenNLP github repo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) and [this AnalyticsVidhya post](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article). In this guide we'll use the python package [Flair](https://github.com/zalandoresearch/flair) to get ELMo embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embeddings with ELMo in Flair\n",
    "In Flair, you init a `Sentence` object given the tokens seperated by spaces.  \n",
    "Sentence has a few useful attributes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "sentence = Sentence('The grass is green .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also init a class of the desired embedding method. The `embed` method of this class gets a Sentence and adds to its tokens the relevant embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "torch.Size([3072])\n",
      "tensor([-0.3288,  0.2022, -0.5940,  ..., -1.2823,  0.3074,  0.2155])\n",
      "Token: 2 grass\n",
      "torch.Size([3072])\n",
      "tensor([ 0.2539, -0.2363,  0.5263,  ..., -0.7016,  0.8834,  1.4157])\n",
      "Token: 3 is\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1915,  0.2300, -0.2894,  ..., -0.3680,  1.9113,  1.4519])\n",
      "Token: 4 green\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1779,  0.1309, -0.1041,  ..., -0.1034,  1.6204,  0.3276])\n",
      "Token: 5 .\n",
      "torch.Size([3072])\n",
      "tensor([-0.8872, -0.2004, -1.0601,  ..., -0.0084, -0.0877,  0.0609])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "elmo_embedding = ELMoEmbeddings()\n",
    "\n",
    "elmo_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Now, compare the embeddings obtained using ELMo for the same word in different contexts. Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word sense disambiguation using ELMo\n",
    "**Try it yourself:** Let's also try to see how ELMo handles word sense disambiguation. Below are 6 sentences with 2 different meanings of the word `bank`. Try to see if ELMo vectors indeed separate the two meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I was walking along the river bank\",\n",
    "    \"I saw a toad near the east bank of the river\",\n",
    "    \"We had a nice picnic by the bank\",\n",
    "    \"I need to deposit money from the bank\",\n",
    "    \"The bank branch is closed\",\n",
    "    \"He started working at the bank\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents using the average of contextual word vectors\n",
    "**Try it yourself:** *Optional:* In previous sections we've built a classifier using the average of non-contextual word vectors. Now, try to use contextual word embeddings on our dataset. Use the average of these vectors and apply a classifier on it to obtain the predictions. Is the performance better than for non-contextual word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embedding using ELMo\n",
    "We've used Flair to get embeddings for each word in the sentence. However, for text classification of the entire document, we need a way to integrate all these vectors into a single document embedding. There are several methods for that, and those interested would find this article useful - https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d\n",
    "\n",
    "The most basic element is averaging the word embedding into a single document embedding. In FLAIR, we do this using a DocumentPooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072])\n",
      "tensor([-0.1185,  0.0253, -0.3043,  ..., -0.4927,  0.9270,  0.6944],\n",
      "       grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([elmo_embedding], pooling='mean')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding().shape)\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use an RNN that runs over the word embeddings. We will use the last hidden state as the document embedding. In this case it is very helpful to train the model using the true labels of our task, so that the RNN is optimized for our own data and task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "document_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while `DocumentPoolEmbeddings` are immediately meaningful, `DocumentRNNEmbeddings` need to be tuned on the downstream task. This happens automatically in Flair if you train a new model with these embeddings. You can find an example of training a text classification model [here](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model). Once the model is trained, you can access the tuned DocumentRNNEmbeddings object directly from the classifier object and use it to embed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DocumentRNNEmbeddings` have a number of hyper-parameters that can be tuned to improve learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ":param hidden_size: the number of hidden states in the rnn.\n",
    ":param rnn_layers: the number of layers for the rnn.\n",
    ":param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
    "layer before putting them into the rnn or not.\n",
    ":param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
    "dimension as before will be taken.\n",
    ":param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not.\n",
    ":param dropout: the dropout value to be used.\n",
    ":param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n",
    ":param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n",
    ":param rnn_type: one of 'RNN' or 'LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "The simplest way to load our data in Flair is using a CSV file. You can learn about other method in [the documentation](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '/path/to/data'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {4: \"text\", 1: \"label_topic\", 2: \"label_subtopic\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, to create a `Corpus` for a text classification task, you need to have three files (train, dev, and test) in the \n",
    "above format located in one folder. This data folder structure could, for example, look like this for the IMDB task:\n",
    "```text\n",
    "/resources/tasks/imdb/train.txt\n",
    "/resources/tasks/imdb/dev.txt\n",
    "/resources/tasks/imdb/test.txt\n",
    "```\n",
    "Now create a `ClassificationCorpus` by pointing to this folder (`/resources/tasks/imdb`). \n",
    "Thereby, each line in a file is converted to a `Sentence` object annotated with the labels.\n",
    "\n",
    "Attention: A text in a line can have multiple sentences. Thus, a `Sentence` object can actually consist of multiple\n",
    "sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = ClassificationCorpus(data_folder,\n",
    "                                      test_file='test.txt',\n",
    "                                      dev_file='dev.txt',\n",
    "                                      train_file='train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our own model\n",
    "We're going to use an RNN to run through the contextual word embeddings we got from ELMo. We will use the hidden state at the end of the document as an embedding for the entire document. We will train the RNN on our labeled dataset, so that the final hidden state carries the most relevant information for our custom classification task.  \n",
    "\n",
    "For more information on training your own model using Flair, see [this tutorial](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change code below to fit our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "\n",
    "# 1. get the corpus\n",
    "corpus: Corpus = TREC_6()\n",
    "\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "\n",
    "                   # comment in flair embeddings for state-of-the-art results\n",
    "                   # FlairEmbeddings('news-forward'),\n",
    "                   # FlairEmbeddings('news-backward'),\n",
    "                   ]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('resources/taggers/ag_news',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)\n",
    "\n",
    "# 8. plot weight traces (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('resources/taggers/ag_news/weights.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Word Vectors with BERT\n",
    "We will later use BERT, a state-of-the-art transformer model that was trained on a very large corpus and can be fine-tuned for our own custom task. The Flair package can also be used to derive contextual word embeddings using BERT and its successors. We will use a different package for BERT, to provide you with sample code for using it, and for adaptation of the weights of the BERT model itself for our own task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can try to use BERT, Roberta, XLNet or other models provided in Flair for contextual word embeddings.  \n",
    "Flair also provides a simple way to stack vectors from different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:38:42,789 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt not found in cache, downloading to C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 73034300/73034300 [00:08<00:00, 8530759.25B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:38:51,700 copying C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb to cache at C:\\Users\\Omri\\.flair\\embeddings\\lm-multi-forward-v0.1.pt\n",
      "2019-10-17 08:38:51,779 removing temp file C:\\Users\\Omri\\AppData\\Local\\Temp\\tmppppabpwb\n",
      "2019-10-17 08:38:52,276 https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt not found in cache, downloading to C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 73034304/73034304 [00:12<00:00, 5879942.68B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17 08:39:05,094 copying C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp to cache at C:\\Users\\Omri\\.flair\\embeddings\\lm-multi-backward-v0.1.pt\n",
      "2019-10-17 08:39:05,172 removing temp file C:\\Users\\Omri\\AppData\\Local\\Temp\\tmptvnijhdp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 995526/995526 [00:00<00:00, 1143581.85B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 521/521 [00:00<00:00, 262364.32B/s]\n",
      "100%|███████████████████████████████████████████████████████████████| 714314041/714314041 [01:08<00:00, 10488502.38B/s]\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings\n",
    "\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-1.4812e-07,  4.5007e-08,  6.0273e-07,  ...,  3.8287e-01,\n",
      "         4.7210e-01,  2.9850e-01])\n",
      "Token: 2 grass\n",
      "tensor([ 1.6254e-04,  1.8764e-07, -7.9038e-09,  ...,  8.5283e-01,\n",
      "        -5.0726e-02,  3.4476e-01])\n",
      "Token: 3 is\n",
      "tensor([-2.4521e-04,  3.4869e-07,  5.5841e-06,  ..., -1.8283e-01,\n",
      "         7.1532e-01,  5.0841e-03])\n",
      "Token: 4 green\n",
      "tensor([8.3005e-05, 4.7261e-08, 5.7315e-07,  ..., 1.0157e+00, 7.5358e-01,\n",
      "        1.1230e-01])\n",
      "Token: 5 .\n",
      "tensor([-8.3244e-07,  1.6451e-07, -1.7201e-08,  ..., -6.0930e-01,\n",
      "         9.0591e-01,  1.7857e-01])\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Train a classifier using stacked embeddings of different models. Do you see an increase in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
