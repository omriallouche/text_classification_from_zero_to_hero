{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/omriallouche/text_classification_from_zero_to_hero.git --depth 1\n",
    "import os\n",
    "os.chdir('text_classification_from_zero_to_hero/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "In this section we will use BERT to classify our documents.\n",
    "We will use the package pytorch-transformers by huggingface, that provides a pytorch implementation of the model.  \n",
    "\n",
    "BERT can be used in different ways:\n",
    "1. Use the model weights pre-trained on a large corpus to predict the labels for our custom task\n",
    "1. Fine-tune the model weights, by unfreezing a certain number of the top layers, thereby better fiting the model to our custom task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [FastBert]() package, that simplifies the use of BERT and similar transformer models, and provides an API in spirit of [fast.ai](https://github.com/fastai/fastai), aiming to expose the most important settings and take care of the rest for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fast_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a DataBunch object\n",
    "The databunch object takes training, validation and test csv files and converts the data into internal representation for BERT, RoBERTa, DistilBERT or XLNet. The object also instantiates the correct data-loaders based on device profile and batch_size and max_sequence_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, we will use the DistilBERT model, from the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108). The model is a 6-layer, 768-hidden, 12-heads, 66M parameters model (compared to BERT Base which is a 12-layer, 768-hidden, 12-heads, 110M parameters model, and BERT large which is a 24-layer, 1024-hidden, 16-heads, 340M parameters model) and trains faster, with lighter memory requirements.\n",
    "Check [this link](https://huggingface.co/transformers/pretrained_models.html) for more info on the available pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'distilbert'\n",
    "tokenizer = 'distilbert-base-uncased'\n",
    "multi_label = False\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "LABEL_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.data_cls import BertDataBunch\n",
    "\n",
    "databunch = BertDataBunch(DATA_PATH, LABEL_PATH,\n",
    "                          tokenizer=tokenizer,\n",
    "                          train_file='train.csv',\n",
    "                          val_file='val.csv',\n",
    "                          label_file='labels.csv',\n",
    "                          text_col='text',\n",
    "                          label_col='label',\n",
    "                          batch_size_per_gpu=16,\n",
    "                          max_seq_length=512,\n",
    "                          multi_gpu=False,\n",
    "                          multi_label=multi_label,\n",
    "                          model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV files should contain the columns `index`, `text` and `label`. In case the column names are different than the usual text and labels, you will have to provide those names in the databunch text_col and label_col parameters.  \n",
    "labels.csv will contain a list of all unique labels, or all possible tags in case of multi-label classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a Learner Object\n",
    "BertLearner is the ‘learner’ object that holds everything together. It encapsulates the key logic for the lifecycle of the model such as training, validation and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from fast_bert.learner_cls import BertLearner\n",
    "from fast_bert.metrics import accuracy\n",
    "import logging\n",
    "\n",
    "OUTPUT_DIR = DATA_PATH\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "learner = BertLearner.from_pretrained_model(\n",
    "                        databunch,\n",
    "                        pretrained_path='distilbert-base-uncased',\n",
    "                        metrics=[{'name': 'accuracy', 'function': accuracy}],\n",
    "                        device=torch.device(\"cuda\"), # for GPU,\n",
    "                        logger=logger,\n",
    "                        output_dir=OUTPUT_DIR,\n",
    "                        finetuned_wgts_path=None,\n",
    "                        warmup_steps=500,\n",
    "                        multi_gpu=True,\n",
    "                        is_fp16=False,\n",
    "                        multi_label=multi_label,\n",
    "                        logging_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| parameter           | description                                                                                                                                                                                                                    |\n",
    "| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| databunch           | Databunch object created earlier                                                                                                                                                                                               |\n",
    "| pretrained_path     | Directory for the location of the pretrained model files or the name of one of the pretrained models i.e. bert-base-uncased, xlnet-large-cased, etc                                                                            |\n",
    "| metrics             | List of metrics functions that you want the model to calculate on the validation set, e.g. accuracy, beta, etc                                                                                                                 |\n",
    "| device              | torch.device of type _cuda_ or _cpu_                                                                                                                                                                                           |\n",
    "| logger              | logger object                                                                                                                                                                                                                  |\n",
    "| output_dir          | Directory for model to save trained artefacts, tokenizer vocabulary and tensorboard files                                                                                                                                      |\n",
    "| finetuned_wgts_path | provide the location for fine-tuned language model (experimental feature)                                                                                                                                                      |\n",
    "| warmup_steps        | number of training warms steps for the scheduler                                                                                                                                                                               |\n",
    "| multi_gpu           | multiple GPUs available e.g. if running on AWS p3.8xlarge instance                                                                                                                                                             |\n",
    "| is_fp16             | FP16 training                                                                                                                                                                                                                  |\n",
    "| multi_label         | multilabel classification                                                                                                                                                                                                      |\n",
    "| logging_steps       | number of steps between each tensorboard metrics calculation. Set it to 0 to disable tensor flow logging. Keeping this value too low will lower the training speed as model will be evaluated each time the metrics are logged |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side-track: View Tensorboard in a Jupyter Notebook\n",
    "Tensorboard provides real-time monitoring of the network training process. To run it in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/text_classification_from_zero_to_hero/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to business - training a BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3553855e232d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m learner.fit(epochs=100,\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6e-5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m      \u001b[1;31m# Evaluate the model after each epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mschedule_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"warmup_cosine\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             optimizer_type=\"lamb\")\n",
      "\u001b[1;31mNameError\u001b[0m: name 'learner' is not defined"
     ]
    }
   ],
   "source": [
    "learner.fit(epochs=100,\n",
    "            lr=6e-5,\n",
    "            validate=True, \t# Evaluate the model after each epoch\n",
    "            schedule_type=\"warmup_cosine\",\n",
    "            optimizer_type=\"lamb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save trained model artifacts\n",
    "Model artefacts will be persisted in the output_dir/'model_out' path provided to the learner object. Following files will be persisted:\n",
    "\n",
    "| File name               | description                                      |\n",
    "| ----------------------- | ------------------------------------------------ |\n",
    "| pytorch_model.bin       | trained model weights                            |\n",
    "| spiece.model            | sentence tokenizer vocabulary (for xlnet models) |\n",
    "| vocab.txt               | workpiece tokenizer vocabulary (for bert models) |\n",
    "| special_tokens_map.json | special tokens mappings                          |\n",
    "| config.json             | model config                                     |\n",
    "| added_tokens.json       | list of new tokens                               |\n",
    "\n",
    "As the model artefacts are all stored in the same folder, you will be able to instantiate the learner object to run inference by pointing pretrained_path to this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform inference, you need to save the model (see step #4 above).  \n",
    "To make predictions, we init a `BertClassificationPredictor` object with the path of the model files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_bert.prediction import BertClassificationPredictor\n",
    "\n",
    "MODEL_PATH = OUTPUT_DIR + 'model_out'\n",
    "\n",
    "predictor = BertClassificationPredictor(\n",
    "                model_path=MODEL_PATH,\n",
    "                label_path=LABEL_PATH, # location for labels.csv file\n",
    "                multi_label=False,\n",
    "                model_type=model_type,\n",
    "                do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single prediction\n",
    "sentence_to_predict = \"the problem with the middleeast is those damn data scientists. All they do is just look at the computer screen instead of laying outside in the warm sun.\"\n",
    "single_prediction = predictor.predict(sentence_to_predict)\n",
    "single_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch predictions\n",
    "texts = [\n",
    "    \"this is the first text\",\n",
    "    \"this is the second text\"\n",
    "    ]\n",
    "\n",
    "multiple_predictions = predictor.predict_batch(texts)\n",
    "multiple_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's run the model on our validation dataset and report its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch predictions\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/val.csv')\n",
    "texts = list(df['text'].values)\n",
    "\n",
    "multiple_predictions = predictor.predict_batch(texts)\n",
    "# Each prediction includes the softmax value of all possible labels, sorting in descending order. \n",
    "# We thus use only the first element, which is the most-probable label, and keep only the first element in that tuple, which is the name of the label\n",
    "y_predicted = [x[0][0] for x in multiple_predictions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "y_truth = df['label']\n",
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Can you improve the performance of our classifier? Let's see how high you can get.  \n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model Fine-Tuning\n",
    "It is also possible to fine-tune the Language Model of BERT, to fit to the custom domain. The process requires a long training time, even on powerful GPUs. Check [this link](https://github.com/kaushaltrivedi/fast-bert#language-model-fine-tuning) for instructions how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
