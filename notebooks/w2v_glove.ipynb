{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/omriallouche/text_classification_from_zero_to_hero.git\n",
    "import os\n",
    "os.chdir('text_classification_from_zero_to_hero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec, GloVe and Word Embeddings\n",
    "## Part 3 of the Workshop \"Text Classification - From Zero to Hero\", by Dr. Omri Allouche, Gong.io, Bar Ilan University\n",
    "\n",
    "For this presentation, we will use FLAIR: https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained word embeddings using Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,\n",
      "        -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,\n",
      "         0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,\n",
      "         0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,\n",
      "         0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,\n",
      "        -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,\n",
      "        -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,\n",
      "         0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,\n",
      "         1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,\n",
      "        -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,\n",
      "         0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,\n",
      "         0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,\n",
      "        -0.5203, -0.1459,  0.8278,  0.2706])\n",
      "Token: 2 grass\n",
      "tensor([-0.8135,  0.9404, -0.2405, -0.1350,  0.0557,  0.3363,  0.0802, -0.1015,\n",
      "        -0.5478, -0.3537,  0.0734,  0.2587,  0.1987, -0.1433,  0.2507,  0.4281,\n",
      "         0.1950,  0.5346,  0.7424,  0.0578, -0.3178,  0.9436,  0.8145, -0.0824,\n",
      "         0.6166,  0.7284, -0.3262, -1.3641,  0.1232,  0.5373, -0.5123,  0.0246,\n",
      "         1.0822, -0.2296,  0.6039,  0.5541, -0.9610,  0.4803,  0.0022,  0.5591,\n",
      "        -0.1637, -0.8468,  0.0741, -0.6216,  0.0260, -0.5162, -0.0525, -0.1418,\n",
      "        -0.0161, -0.4972, -0.5534, -0.4037,  0.5096,  1.0276, -0.0840, -1.1179,\n",
      "         0.3226,  0.4928,  0.9488,  0.2040,  0.5388,  0.8397, -0.0689,  0.3136,\n",
      "         1.0450, -0.2267, -0.0896, -0.6427,  0.6443, -1.1001, -0.0096,  0.2668,\n",
      "        -0.3230, -0.6065,  0.0479, -0.1664,  0.8571,  0.2335,  0.2539,  1.2546,\n",
      "         0.5472, -0.1980, -0.7186,  0.2076, -0.2587, -0.3650,  0.0834,  0.6932,\n",
      "         0.1574,  1.0931,  0.0913, -1.3773, -0.2717,  0.7071,  0.1872, -0.3307,\n",
      "        -0.2836,  0.1030,  1.2228,  0.8374])\n",
      "Token: 3 is\n",
      "tensor([-0.5426,  0.4148,  1.0322, -0.4024,  0.4669,  0.2182, -0.0749,  0.4733,\n",
      "         0.0810, -0.2208, -0.1281, -0.1144,  0.5089,  0.1157,  0.0282, -0.3628,\n",
      "         0.4382,  0.0475,  0.2028,  0.4986, -0.1007,  0.1327,  0.1697,  0.1165,\n",
      "         0.3135,  0.2571,  0.0928, -0.5683, -0.5297, -0.0515, -0.6733,  0.9253,\n",
      "         0.2693,  0.2273,  0.6636,  0.2622,  0.1972,  0.2609,  0.1877, -0.3454,\n",
      "        -0.4263,  0.1398,  0.5634, -0.5691,  0.1240, -0.1289,  0.7248, -0.2610,\n",
      "        -0.2631, -0.4360,  0.0789, -0.8415,  0.5160,  1.3997, -0.7646, -3.1453,\n",
      "        -0.2920, -0.3125,  1.5129,  0.5243,  0.2146,  0.4245, -0.0884, -0.1780,\n",
      "         1.1876,  0.1058,  0.7657,  0.2191,  0.3582, -0.1164,  0.0933, -0.6248,\n",
      "        -0.2190,  0.2180,  0.7406, -0.4374,  0.1434,  0.1472, -1.1605, -0.0505,\n",
      "         0.1268, -0.0144, -0.9868, -0.0913, -1.2054, -0.1197,  0.0478, -0.5400,\n",
      "         0.5246, -0.7096, -0.3253, -0.1346, -0.4131,  0.3343, -0.0072,  0.3225,\n",
      "        -0.0442, -1.2969,  0.7622,  0.4635])\n",
      "Token: 4 green\n",
      "tensor([-6.7907e-01,  3.4908e-01, -2.3984e-01, -9.9652e-01,  7.3782e-01,\n",
      "        -6.5911e-04,  2.8010e-01,  1.7287e-02, -3.6063e-01,  3.6955e-02,\n",
      "        -4.0395e-01,  2.4092e-02,  2.8958e-01,  4.0497e-01,  6.9992e-01,\n",
      "         2.5269e-01,  8.0350e-01,  4.9370e-02,  1.5562e-01, -6.3286e-03,\n",
      "        -2.9414e-01,  1.4728e-01,  1.8977e-01, -5.1791e-01,  3.6986e-01,\n",
      "         7.4582e-01,  8.2689e-02, -7.2601e-01, -4.0939e-01, -9.7822e-02,\n",
      "        -1.4096e-01,  7.1121e-01,  6.1933e-01, -2.5014e-01,  4.2250e-01,\n",
      "         4.8458e-01, -5.1915e-01,  7.7125e-01,  3.6685e-01,  4.9652e-01,\n",
      "        -4.1298e-02, -1.4683e+00,  2.0038e-01,  1.8591e-01,  4.9860e-02,\n",
      "        -1.7523e-01, -3.5528e-01,  9.4153e-01, -1.1898e-01, -5.1903e-01,\n",
      "        -1.1887e-02, -3.9186e-01, -1.7479e-01,  9.3451e-01, -5.8931e-01,\n",
      "        -2.7701e+00,  3.4522e-01,  8.6533e-01,  1.0808e+00, -1.0291e-01,\n",
      "        -9.1220e-02,  5.5092e-01, -3.9473e-01,  5.3676e-01,  1.0383e+00,\n",
      "        -4.0658e-01,  2.4590e-01, -2.6797e-01, -2.6036e-01, -1.4151e-01,\n",
      "        -1.2022e-01,  1.6234e-01, -7.4320e-01, -6.4728e-01,  4.7133e-02,\n",
      "         5.1642e-01,  1.9898e-01,  2.3919e-01,  1.2550e-01,  2.2471e-01,\n",
      "         8.2613e-01,  7.8328e-02, -5.7020e-01,  2.3934e-02, -1.5410e-01,\n",
      "        -2.5739e-01,  4.1262e-01, -4.6967e-01,  8.7914e-01,  7.2629e-01,\n",
      "         5.3862e-02, -1.1575e+00, -4.7835e-01,  2.0139e-01, -1.0051e+00,\n",
      "         1.1515e-01, -9.6609e-01,  1.2960e-01,  1.8388e-01, -3.0383e-02])\n",
      "Token: 5 .\n",
      "tensor([-0.3398,  0.2094,  0.4635, -0.6479, -0.3838,  0.0380,  0.1713,  0.1598,\n",
      "         0.4662, -0.0192,  0.4148, -0.3435,  0.2687,  0.0446,  0.4213, -0.4103,\n",
      "         0.1546,  0.0222, -0.6465,  0.2526,  0.0431, -0.1945,  0.4652,  0.4565,\n",
      "         0.6859,  0.0913,  0.2188, -0.7035,  0.1679, -0.3508, -0.1263,  0.6638,\n",
      "        -0.2582,  0.0365, -0.1361,  0.4025,  0.1429,  0.3813, -0.1228, -0.4589,\n",
      "        -0.2528, -0.3043, -0.1121, -0.2618, -0.2248, -0.4455,  0.2991, -0.8561,\n",
      "        -0.1450, -0.4909,  0.0083, -0.1749,  0.2752,  1.4401, -0.2124, -2.8435,\n",
      "        -0.2796, -0.4572,  1.6386,  0.7881, -0.5526,  0.6500,  0.0864,  0.3901,\n",
      "         1.0632, -0.3538,  0.4833,  0.3460,  0.8417,  0.0987, -0.2421, -0.2705,\n",
      "         0.0453, -0.4015,  0.1139,  0.0062,  0.0367,  0.0185, -1.0213, -0.2081,\n",
      "         0.6407, -0.0688, -0.5864,  0.3348, -1.1432, -0.1148, -0.2509, -0.4591,\n",
      "        -0.0968, -0.1795, -0.0634, -0.6741, -0.0689,  0.5360, -0.8777,  0.3180,\n",
      "        -0.3924, -0.2339,  0.4730, -0.0288])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import Sentence, WordEmbeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "glove_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Compare the embeddings obtained using GloVe for the same word in different context (ie different sentences). Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embedding using the average of word vectors\n",
    "Now, let's average the vectors into a single vector that would represent our entire document, and use it for classification. We'll build a Logistic Regression classifier on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sentence_embedding(sentence):\n",
    "    sentence = Sentence(sentence)\n",
    "    glove_embedding.embed(sentence)\n",
    "    sentence_embedding = np.mean( [np.array(token.embedding) for token in sentence], axis=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.48264474,  0.33375996,  0.348696  , -0.5163    ,  0.191962  ,\n",
       "        0.12714759,  0.013061  ,  0.1766614 , -0.1873308 , -0.093839  ,\n",
       "        0.0488024 , -0.0484856 ,  0.314986  ,  0.031634  ,  0.2535662 ,\n",
       "       -0.059972  ,  0.38505   ,  0.06304   ,  0.027378  ,  0.06385148,\n",
       "       -0.1046188 ,  0.131214  ,  0.39698398,  0.0049592 ,  0.48706597,\n",
       "        0.27059498,  0.0188544 , -0.780686  , -0.160654  , -0.0207716 ,\n",
       "       -0.2985124 ,  0.521548  ,  0.371312  ,  0.0037584 ,  0.24874802,\n",
       "        0.3579286 , -0.187218  ,  0.484008  ,  0.1211252 ,  0.0338024 ,\n",
       "       -0.32039762, -0.578998  ,  0.1858078 , -0.27883598,  0.07773139,\n",
       "       -0.14281002,  0.23905559, -0.13043599, -0.1817726 , -0.49833995,\n",
       "       -0.10820474, -0.30922002,  0.285602  ,  1.1599319 , -0.49102196,\n",
       "       -2.58022   ,  0.021746  ,  0.043806  ,  1.479552  ,  0.427112  ,\n",
       "       -0.02804599,  0.67730397, -0.0862168 ,  0.305978  ,  1.0884    ,\n",
       "       -0.21497002,  0.2661428 , -0.022402  ,  0.3063696 , -0.2959406 ,\n",
       "       -0.04430168, -0.124852  , -0.3095786 , -0.37071198,  0.2658556 ,\n",
       "        0.01379652,  0.1408166 ,  0.0865896 , -0.61099005,  0.25847322,\n",
       "        0.569286  ,  0.05893   , -0.656514  ,  0.1472954 , -0.85989   ,\n",
       "       -0.23183   ,  0.043911  , -0.21173999,  0.36706018,  0.13562599,\n",
       "       -0.0454518 , -0.6721238 , -0.32438502,  0.53062   , -0.48571223,\n",
       "       -0.01712   , -0.4413198 , -0.288836  ,  0.69392604,  0.3024668 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_embedding('The grass is green .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "clf = linear_model.LogisticRegression(C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/train.csv')\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "clf.fit(vectors, y_truth)\n",
    "\n",
    "y_predict = clf.predict(vectors)\n",
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's check the performance on our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7087812901155326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/val.csv')\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "y_predict = clf.predict(vectors)\n",
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   rec.sport.baseball       0.69      0.69      0.69        16\n",
      "     rec.sport.hockey       0.71      0.75      0.73        20\n",
      "   talk.politics.guns       0.79      0.68      0.73        22\n",
      "talk.politics.mideast       0.65      0.72      0.68        18\n",
      "\n",
      "            micro avg       0.71      0.71      0.71        76\n",
      "            macro avg       0.71      0.71      0.71        76\n",
      "         weighted avg       0.72      0.71      0.71        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_truth, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding using Flair's DocumentPoolEmbeddings Class\n",
    "We've used Flair to get embeddings for each word in the sentence. However, for text classification of the entire document, we need a way to integrate all these vectors into a single document embedding. There are several methods for that, and those interested would find this article useful - https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d\n",
    "\n",
    "The most basic element is averaging the word embedding into a single document embedding. In FLAIR, we do this using `DocumentPoolEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentPoolEmbeddings\n",
    "document_embeddings = DocumentPoolEmbeddings([glove_embedding], pooling='mean')\n",
    "\n",
    "document_embeddings.embed(sentence)\n",
    "document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding().shape)\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By using an RNN\n",
    "Alternatively, we can use an RNN that runs over the word embeddings. We will use the last hidden state as the document embedding. In this case it is very helpful to train the model using the true labels of our task, so that the RNN is optimized for our own data and task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "document_embeddings = DocumentRNNEmbeddings([glove_embedding], rnn_type='LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an example sentence\n",
    "sentence = Sentence('The grass is green . And the sky is blue .')\n",
    "\n",
    "# embed the sentence with our document embedding\n",
    "document_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded sentence.\n",
    "print(sentence.get_embedding())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while `DocumentPoolEmbeddings` are immediately meaningful, `DocumentRNNEmbeddings` need to be tuned on the downstream task. This happens automatically in Flair if you train a new model with these embeddings. You can find an example of training a text classification model [here](/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md#training-a-text-classification-model). Once the model is trained, you can access the tuned DocumentRNNEmbeddings object directly from the classifier object and use it to embed sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DocumentRNNEmbeddings` have a number of hyper-parameters that can be tuned to improve learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ":param hidden_size: the number of hidden states in the rnn.\n",
    ":param rnn_layers: the number of layers for the rnn.\n",
    ":param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
    "layer before putting them into the rnn or not.\n",
    ":param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
    "dimension as before will be taken.\n",
    ":param bidirectional: boolean value, indicating whether to use a bidirectional rnn or not.\n",
    ":param dropout: the dropout value to be used.\n",
    ":param word_dropout: the word dropout value to be used, if 0.0 word dropout is not used.\n",
    ":param locked_dropout: the locked dropout value to be used, if 0.0 locked dropout is not used.\n",
    ":param rnn_type: one of 'RNN' or 'LSTM'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training RNN on our own dataset - Loading dataset\n",
    "The simplest way to load our data in Flair is using a CSV file. You can learn about other method in [the documentation](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_6_CORPUS.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Corpus` for a text classification task, you need to have three files (train, dev, and test) in the \n",
    "above format located in one folder. This data folder structure could, for example, look like this for the IMDB task:\n",
    "```text\n",
    "/data/train.csv\n",
    "/data/val.csv\n",
    "/data/test.txt\n",
    "```\n",
    "Now create a `CSVClassificationCorpus` by pointing to this folder (`/data`). \n",
    "Thereby, each line in a file is converted to a `Sentence` object annotated with the labels.\n",
    "\n",
    "Attention: A text in a line can in fact have multiple sentences. Thus, a `Sentence` object is actually a `Document` and can actually consist of multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'data/'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s). This is 1-based and not 0-based\n",
    "column_name_map = {3: \"text\", 2: \"label\"}\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                      test_file='test.csv',\n",
    "                                      dev_file='val.csv',\n",
    "                                      train_file='train.csv')\n",
    "    \n",
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training RNN on our own dataset - Training our own model\n",
    "We're going to use an RNN to run through the contextual word embeddings we got from ELMo. We will use the hidden state at the end of the document as an embedding for the entire document. We will train the RNN on our labeled dataset, so that the final hidden state carries the most relevant information for our custom classification task.  \n",
    "\n",
    "For more information on training your own model using Flair, see [this tutorial](https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_7_TRAINING_A_MODEL.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change code below to fit our own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [WordEmbeddings('glove')]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=256,\n",
    "                                                                     )\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "trainer.train('data/',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=5,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. plot weight traces (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_weights('data/weights.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
