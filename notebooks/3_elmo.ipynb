{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/omriallouche/text_classification_from_zero_to_hero.git --depth 1\n",
    "import os\n",
    "os.chdir('text_classification_from_zero_to_hero/notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings with ELMo\n",
    "## Part 3 of the Workshop \"Text Classification - From Zero to Hero\", by Dr. Omri Allouche, Gong.io, Bar Ilan University\n",
    "\n",
    "ELMo learns contextualized word vectors by running the text through a deep recurrent network.  \n",
    "ELMo is actually an algorithm for unsupervised learning and does not make any use of the labels we have for our text classification task. The authors do show that contextualized word vectors obtained using ELMo increase text classification performance in a large array of tasks. Let's see if we see a significant gain in our case!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are good examples of using ELMo in both [the AllenNLP github repo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) and [this AnalyticsVidhya post](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article). In this guide we'll use the python package [Flair](https://github.com/zalandoresearch/flair) to get ELMo embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embeddings with ELMo in Flair\n",
    "In Flair, you init a `Sentence` object given the tokens seperated by spaces.  \n",
    "Sentence has a few useful attributes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "sentence = Sentence('The grass is green .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also init a class of the desired embedding method. The `embed` method of this class gets a Sentence and adds to its tokens the relevant embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "torch.Size([3072])\n",
      "tensor([-0.3288,  0.2022, -0.5940,  ..., -1.2773,  0.3049,  0.2150])\n",
      "Token: 2 grass\n",
      "torch.Size([3072])\n",
      "tensor([ 0.2539, -0.2363,  0.5263,  ..., -0.7001,  0.8798,  1.4191])\n",
      "Token: 3 is\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1915,  0.2300, -0.2894,  ..., -0.3626,  1.9066,  1.4520])\n",
      "Token: 4 green\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1779,  0.1309, -0.1041,  ..., -0.1006,  1.6152,  0.3299])\n",
      "Token: 5 .\n",
      "torch.Size([3072])\n",
      "tensor([-0.8872, -0.2004, -1.0601,  ..., -0.0106, -0.0833,  0.0669])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "elmo_embedding = ELMoEmbeddings()\n",
    "\n",
    "elmo_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Now, compare the embeddings obtained using ELMo for the same word in different contexts. Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Word sense disambiguation using ELMo\n",
    "**Try it yourself:** Let's also try to see how ELMo handles word sense disambiguation. Below are 6 sentences with 2 different meanings of the word `bank`. Try to see if ELMo vectors indeed separate the two meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I was walking along the river bank\",\n",
    "    \"I saw a toad near the east bank of the river\",\n",
    "    \"We had a nice picnic by the bank\",\n",
    "    \"I need to deposit money from the bank\",\n",
    "    \"The bank branch is closed\",\n",
    "    \"He started working at the bank\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents using the average of contextual word vectors\n",
    "**Try it yourself:** *Optional:* In previous sections we've built a classifier using the average of non-contextual word vectors. Now, try to use contextual word embeddings on our dataset. Use the average of these vectors and apply a classifier on it to obtain the predictions. Is the performance better than for non-contextual word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "clf = linear_model.LogisticRegression(C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sentence_embedding(sentence):\n",
    "    sentence = Sentence(sentence)\n",
    "    elmo_embedding.embed(sentence)\n",
    "    sentence_embedding = np.mean( [np.array(token.embedding) for token in sentence], axis=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='warn', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training set\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/train.csv')\n",
    "# Compute sentence embedding for the dataset\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "clf.fit(vectors, y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.753968253968254"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test set\n",
    "df = pd.read_csv('../data/val.csv')\n",
    "# Compute sentence embedding for the dataset\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "y_predict = clf.predict(vectors)\n",
    "metrics.accuracy_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "   rec.sport.baseball       0.71      0.62      0.67        16\n",
      "     rec.sport.hockey       0.73      0.80      0.76        20\n",
      "   talk.politics.guns       0.83      0.86      0.84        22\n",
      "talk.politics.mideast       0.76      0.72      0.74        18\n",
      "\n",
      "            micro avg       0.76      0.76      0.76        76\n",
      "            macro avg       0.76      0.75      0.75        76\n",
      "         weighted avg       0.76      0.76      0.76        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_truth, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
