{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings with ELMo\n",
    "## Part 4 of the Workshop \"Text Classification - From Zero to Hero\", by Dr. Omri Allouche, Gong.io, Bar Ilan University\n",
    "\n",
    "ELMo learns contextualized word vectors by running the text through a deep recurrent network.  \n",
    "ELMo is actually an algorithm for unsupervised learning and does not make any use of the labels we have for our text classification task. The authors do show that contextualized word vectors obtained using ELMo increase text classification performance in a large array of tasks. Let's see if we see a significant gain in our case!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are good examples of using ELMo in both [the AllenNLP github repo](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) and [this AnalyticsVidhya post](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=top-pretrained-models-nlp-article). In this guide we'll use the python package [Flair](https://github.com/zalandoresearch/flair) to get ELMo embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "    \n",
    "if 'google.colab' in sys.modules:\n",
    "    project_folder = \"/content/drive/My Drive/nlpday_content/zero2hero/\"\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    __dir__ = project_folder\n",
    "    sys.path.append(__dir__ + '/src')\n",
    "\n",
    "    def locate(fname):\n",
    "      \"\"\"Search file in google drive\"\"\"\n",
    "      if os.path.exists(fname):\n",
    "        return fname\n",
    "      try:\n",
    "        return next(filter(lambda p: str(p).endswith(fname),\n",
    "                           Path(project_folder).glob('**/*.*')))\n",
    "      except StopIteration:\n",
    "        raise FileNotFoundError(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual word embeddings with ELMo in Flair\n",
    "In Flair, you init a `Sentence` object given the tokens seperated by spaces.  \n",
    "Sentence has a few useful attributes and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install allennlp\n",
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import Sentence\n",
    "sentence = Sentence('The grass is green .')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also init a class of the desired embedding method. The `embed` method of this class gets a Sentence and adds to its tokens the relevant embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: 1 The\n",
      "torch.Size([3072])\n",
      "tensor([-0.3288,  0.2022, -0.5940,  ..., -1.2773,  0.3049,  0.2150])\n",
      "Token: 2 grass\n",
      "torch.Size([3072])\n",
      "tensor([ 0.2539, -0.2363,  0.5263,  ..., -0.7001,  0.8798,  1.4191])\n",
      "Token: 3 is\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1915,  0.2300, -0.2894,  ..., -0.3626,  1.9066,  1.4520])\n",
      "Token: 4 green\n",
      "torch.Size([3072])\n",
      "tensor([ 0.1779,  0.1309, -0.1041,  ..., -0.1006,  1.6152,  0.3299])\n",
      "Token: 5 .\n",
      "torch.Size([3072])\n",
      "tensor([-0.8872, -0.2004, -1.0601,  ..., -0.0106, -0.0833,  0.0669])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings\n",
    "\n",
    "# init embedding\n",
    "elmo_embedding = ELMoEmbeddings()\n",
    "\n",
    "elmo_embedding.embed(sentence)\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding.shape)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Now, compare the embeddings obtained using ELMo for the same word in different contexts. Are they equal or different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word sense disambiguation using ELMo\n",
    "**Try it yourself:** Let's also try to see how ELMo handles word sense disambiguation. Below are 6 sentences with 2 different meanings of the word `bank`. Try to see if ELMo vectors indeed separate the two meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I was walking along the river bank\",\n",
    "    \"I saw a toad near the east bank of the river\",\n",
    "    \"We had a nice picnic by the bank\",\n",
    "    \"I need to deposit money from the bank\",\n",
    "    \"The bank branch is closed\",\n",
    "    \"He started working at the bank\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embedding and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents using the average of contextual word vectors\n",
    "**Try it yourself:** *Optional:* In previous sections we've built a classifier using the average of non-contextual word vectors. Now, try to use contextual word embeddings on our dataset. Use the average of these vectors and apply a classifier on it to obtain the predictions. Is the performance better than for non-contextual word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "clf = linear_model.LogisticRegression(C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sentence_embedding(sentence):\n",
    "    sentence = Sentence(sentence)\n",
    "    elmo_embedding.embed(sentence)\n",
    "    sentence_embedding = np.mean( [np.array(token.embedding) for token in sentence], axis=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training set\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# Compute sentence embedding for the dataset\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "clf.fit(vectors, y_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test set\n",
    "df = pd.read_csv('data/val.csv')\n",
    "# Compute sentence embedding for the dataset\n",
    "vectors = np.array([get_sentence_embedding(x) for x in df['text']])\n",
    "y_truth = df['label']\n",
    "y_predict = clf.predict(vectors)\n",
    "metrics.f1_score(y_truth, y_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_truth, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Word Vectors with BERT and Stacking Embeddings\n",
    "We will later use BERT, a state-of-the-art transformer model that was trained on a very large corpus and can be fine-tuned for our own custom task. The Flair package can also be used to derive contextual word embeddings using BERT and its successors. We will use a different package for BERT, to provide you with sample code for using it, and for adaptation of the weights of the BERT model itself for our own task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can try to use BERT, Roberta, XLNet or other models provided in Flair for contextual word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import BertEmbeddings\n",
    "# init BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Embeddings in Flair\n",
    "Flair also provides a simple way to stack vectors from different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import FlairEmbeddings, BertEmbeddings\n",
    "\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('multi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('multi-backward')\n",
    "\n",
    "# init BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = Sentence('The grass is green .')\n",
    "\n",
    "# just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "stacked_embeddings.embed(sentence)\n",
    "\n",
    "# now check out the embedded tokens.\n",
    "for token in sentence:\n",
    "    print(token)\n",
    "    print(token.embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try it yourself:** Train a classifier using stacked embeddings of different models. Do you see an increase in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
